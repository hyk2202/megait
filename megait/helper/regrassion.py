import numpy as np
import seaborn as sb
from matplotlib import pyplot as plt

from tabulate import tabulate        
from pandas import DataFrame, Series

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.stattools import durbin_watson

from scipy.stats import t, f 
from helper.util import my_pretty_table

def my_linear_regrassion(x_train : DataFrame, y_train : Series, x_test : DataFrame, y_test : Series, use_plot : bool = True, report=True) -> LinearRegression :
    """ì„ í˜•íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°
        use_plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.

    Returns:
        LinearRegression: íšŒê·€ë¶„ì„ ëª¨ë¸
    """
    xnames = x_train.columns
    yname = y_train.name
    size = len(xnames)

    # ë¶„ì„ëª¨ë¸ ìƒì„±
    model = LinearRegression(n_jobs=-1) # n_jobs : ì‚¬ìš©í•˜ëŠ” cpu ì½”ì–´ì˜ ê°œìˆ˜ // -1ì€ ìµœëŒ€ì¹˜
    fit = model.fit(x_train, y_train)

    expr = f"{yname} = "

    for i, v in enumerate(xnames):
        expr += f"{fit.coef_[i]:0.3f} * {v} + " 

    expr += f"{fit.intercept_:0.3f}" 
    print("[íšŒê·€ì‹]")
    print(expr, end="\n\n")

    # ì„±ëŠ¥ì§€í‘œ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    result_data = []
    y_train_pred = fit.predict(x_train)
    y_test_pred = fit.predict(x_test)
    target = [[x_train, y_train, y_train_pred], [x_test, y_test, y_test_pred]]

    for i, v in enumerate(target):
        result = {
            "ê²°ì •ê³„ìˆ˜(R2)": r2_score(v[1], v[2]),
            "í‰ê· ì ˆëŒ€ì˜¤ì°¨(MAE)": mean_absolute_error(v[1], v[2]),
            "í‰ê· ì œê³±ì˜¤ì°¨(MSE)": mean_squared_error(v[1], v[2]),
            "í‰ê· ì˜¤ì°¨(RMSE)": np.sqrt(mean_squared_error(v[1], v[2])),
            "í‰ê·  ì ˆëŒ€ ë°±ë¶„ì˜¤ì°¨ ë¹„ìœ¨(MAPE)": np.mean(np.abs((v[1] - v[2]) / v[1]) * 100),
            "í‰ê·  ë¹„ìœ¨ ì˜¤ì°¨(MPE)": np.mean((v[1] - v[2]) / v[1] * 100)
        }
        result_data.append(result)
    
    result_df = DataFrame(result_data, index=["í›ˆë ¨ë°ì´í„°", "ê²€ì¦ë°ì´í„°"])
    my_pretty_table(result_df)
    
    if report:
        my_linear_regrassion_report(fit, x_train, y_train, x_test, y_test)

    if use_plot:
        for i,v in enumerate(xnames):
            fig, ax = plt.subplots(1, 2, figsize=(15, 4), dpi=150)
            fig.subplots_adjust(hspace=0.3)
            for j,w in enumerate(target):
                sb.regplot(x=w[0][v], y=w[1], ci=95, ax=ax[j], label='ê´€ì¸¡ì¹˜')
                sb.regplot(x=w[0][v], y=w[1], ci=0, ax=ax[j], label='ì¶”ì •ì¹˜')
                ax[j].set_title(f"{'í›ˆë ¨ë°ì´í„°' if j == 0 else 'ê²€ì¦ë°ì´í„°'}: {yname} vs {v}")
                ax[j].legend()
                ax[j].grid()

            plt.show()
            plt.close()

    return fit

def my_linear_regrassion_report(fit : LinearRegression, x_train : DataFrame, y_train : Series, x_test : DataFrame, y_test: Series) -> None:
    """ì„ í˜•íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ í•œë‹¤.    

    Args:
        fit (LinearRegression): ì„ í˜•íšŒê·€ ê°ì²´
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°
    """

    xnames = x_train.columns
    yname = y_train.name
    size = len(xnames)

    y_train_pred = fit.predict(x_train)
    y_test_pred = fit.predict(x_test)
    target = [[x_train, y_train, y_train_pred], [x_test, y_test, y_test_pred]]
    for i, v in enumerate(target):
        print(f"[{'í›ˆë ¨' if i == 0 else 'ê²€ì¦'}ë°ì´í„°ì— ëŒ€í•œ ê²°ê³¼ë³´ê³ ]")
        
        target_x, target_y, target_y_pred = v
        
        # ì”ì°¨
        resid = target_y - target_y_pred

        # ì ˆí¸ê³¼ ê³„ìˆ˜ë¥¼ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•©
        params = np.append(fit.intercept_, fit.coef_)

        # ê²€ì¦ìš© ë…ë¦½ë³€ìˆ˜ì— ìƒìˆ˜í•­ ì¶”ê°€
        design_x = target_x.copy()
        design_x.insert(0, 'ìƒìˆ˜', 1)

        dot = np.dot(design_x.T,design_x)   # í–‰ë ¬ê³±
        inv = np.linalg.inv(dot)            # ì—­í–‰ë ¬
        dia = inv.diagonal()                # ëŒ€ê°ì›ì†Œ

        # ì œê³±ì˜¤ì°¨
        MSE = (sum((target_y-target_y_pred)**2)) / (len(design_x)-len(design_x.iloc[0]))

        se_b = np.sqrt(MSE * dia)           # í‘œì¤€ì˜¤ì°¨
        ts_b = params / se_b                # tê°’

        # ê° ë…ë¦½ìˆ˜ì— ëŒ€í•œ pvalue
        p_values = [2*(1-t.cdf(np.abs(i),(len(design_x)-len(design_x.iloc[0])))) for i in ts_b]

        # VIF
        vif = [variance_inflation_factor(target_x, list(target_x.columns).index(v)) for v in target_x.columns]

        # í‘œì¤€í™” ê³„ìˆ˜
        train_df = target_x.copy()
        train_df[target_y.name] = target_y
        scaler = StandardScaler()
        std = scaler.fit_transform(train_df)
        std_df = DataFrame(std, columns=train_df.columns)
        std_x = std_df[xnames]
        std_y = std_df[yname]
        std_model = LinearRegression()
        std_fit = std_model.fit(std_x, std_y)
        beta = std_fit.coef_

        # ê²°ê³¼í‘œ êµ¬ì„±í•˜ê¸°
        result_df = DataFrame({
            "ì¢…ì†ë³€ìˆ˜": [yname] * size,
            "ë…ë¦½ë³€ìˆ˜": xnames,
            "B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)": np.round(params[1:], 4),
            "í‘œì¤€ì˜¤ì°¨": np.round(se_b[1:], 3),
            "Î²(í‘œì¤€í™” ê³„ìˆ˜)": np.round(beta, 3),
            "t": np.round(ts_b[1:], 3),
            "ìœ ì˜í™•ë¥ ": np.round(p_values[1:], 3),
            "VIF": vif,
        })

        #result_df
        my_pretty_table(result_df)
        
        resid = target_y - target_y_pred        # ì”ì°¨
        dw = durbin_watson(resid)               # ë”ë¹ˆ ì™“ìŠ¨ í†µê³„ëŸ‰
        r2 = r2_score(target_y, target_y_pred)  # ê²°ì •ê³„ìˆ˜(ì„¤ëª…ë ¥)
        rowcount = len(target_x)                # í‘œë³¸ìˆ˜
        featurecount = len(target_x.columns)    # ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜

        # ë³´ì •ëœ ê²°ì •ê³„ìˆ˜
        adj_r2 = 1 - (1 - r2) * (rowcount-1) / (rowcount-featurecount-1)

        # fê°’
        f_statistic = (r2 / featurecount) / ((1 - r2) / (rowcount - featurecount - 1))

        # Prob (F-statistic)
        p = 1 - f.cdf(f_statistic, featurecount, rowcount - featurecount - 1)

        tpl = f"ğ‘…^2({r2:.3f}), Adj.ğ‘…^2({adj_r2:.3f}), F({f_statistic:.3f}), P-value({p:.4g}), Durbin-Watson({dw:.3f})"
        print(tpl, end="\n\n")

        # ê²°ê³¼ë³´ê³ 
        tpl = f"{yname}ì— ëŒ€í•˜ì—¬ {','.join(xnames)}ë¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼, ì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜{'í•˜ë‹¤' if p <= 0.05 else 'í•˜ì§€ ì•Šë‹¤'}(F({len(target_x.columns)},{len(target_x.index)-len(target_x.columns)-1}) = {f_statistic:0.3f}, p {'<=' if p <= 0.05 else '>'} 0.05)."

        print(tpl, end = '\n\n')

        # ë…ë¦½ë³€ìˆ˜ ë³´ê³ 
        for n in xnames:
            item = result_df[result_df['ë…ë¦½ë³€ìˆ˜'] == n]
            coef = item['B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)'].values[0]
            pvalue = item['ìœ ì˜í™•ë¥ '].values[0]

            s = f"{n}ì˜ íšŒê·€ê³„ìˆ˜ëŠ” {coef:0.3f}(p {'<=' if pvalue <= 0.05 else '>'} 0.05)ë¡œ, {yname}ì— ëŒ€í•˜ì—¬ {'ìœ ì˜ë¯¸í•œ' if pvalue <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•Šì€'} ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤."

            print(s)
            
        print("")
        