import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt

from tabulate import tabulate
from pandas import DataFrame, Series

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.stattools import durbin_watson
from statsmodels.stats.api import het_breuschpagan
from sklearn.model_selection import GridSearchCV

from scipy.stats import t, f
from helper.util import my_pretty_table, my_trend
from helper.plot import my_residplot, my_qqplot

def my_linear_regrassion(x_train: DataFrame, y_train: Series, x_test: DataFrame = None, y_test: DataFrame = None, cv: int = 0, degree : int = 1,use_plot: bool = True, report=True, resid_test=False, figsize=(10, 4), dpi=150, order: str = None) -> LinearRegression:
    """ì„ í˜•íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        use_plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 4).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 150.
        order (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
    Returns:
        LinearRegression: íšŒê·€ë¶„ì„ ëª¨ë¸
    """
    xnames = x_train.columns
    yname = y_train.name
    size = len(xnames)

    # ë¶„ì„ëª¨ë¸ ìƒì„±
    model = LinearRegression(n_jobs=-1) # n_jobs : ì‚¬ìš©í•˜ëŠ” cpu ì½”ì–´ì˜ ê°œìˆ˜ // -1ì€ ìµœëŒ€ì¹˜

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv > 0:
        params = {}
        grid = GridSearchCV(model, param_grid=params, cv=cv, n_jobs=-1)
        fit = grid.fit(x_train, y_train)
        model = fit.best_estimator_
        fit.best_params = fit.best_params_
        
        result_df = DataFrame(grid.cv_results_['params'])
        result_df['mean_test_score'] = grid.cv_results_['mean_test_score']
        
        print("[êµì°¨ê²€ì¦]")
        my_pretty_table(result_df.sort_values(by='mean_test_score', ascending=False))
        print("")

    fit = model.fit(x_train, y_train)

    expr = f"{yname} = "

    for i, v in enumerate(xnames):
        expr += f"{fit.coef_[i]:0.3f} * {v} + " 

    expr += f"{fit.intercept_:0.3f}" 
    print("[íšŒê·€ì‹]")
    print(expr, end="\n\n")

    if x_test is not None and y_test is not None:
        my_linear_regrassion_result(fit, x_test, y_test, degree, use_plot, report, resid_test, figsize, dpi, order)
    else:
        my_linear_regrassion_result(fit, x_train, y_train, degree, use_plot, report, resid_test, figsize, dpi, order)

    return fit

def my_linear_regrassion_result(fit: LinearRegression, x: DataFrame, y: Series, degree: int = 1,use_plot: bool = True, report=True, resid_test=False, figsize=(10, 4), dpi=150) -> LinearRegression:
    """ì„ í˜•íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        fit (LinearRegression): íšŒê·€ë¶„ì„ ëª¨ë¸
        x (DataFrame): ë…ë¦½ë³€ìˆ˜
        y (Series): ì¢…ì†ë³€ìˆ˜
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        use_plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 4).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 150.

    Returns:
        LinearRegression: íšŒê·€ë¶„ì„ ëª¨ë¸
    """
    xnames = x.columns
    yname = y.name
    
    # í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ì¶”ì •ì¹˜ ìƒì„±
    y_pred = fit.predict(x)

    # ì„±ëŠ¥í‰ê°€
    result = {
        "ê²°ì •ê³„ìˆ˜(R2)": r2_score(y, y_pred),
        "í‰ê· ì ˆëŒ€ì˜¤ì°¨(MAE)": mean_absolute_error(y, y_pred),
        "í‰ê· ì œê³±ì˜¤ì°¨(MSE)": mean_squared_error(y, y_pred),
        "í‰ê· ì˜¤ì°¨(RMSE)": np.sqrt(mean_squared_error(y, y_pred)),
        "í‰ê·  ì ˆëŒ€ ë°±ë¶„ì˜¤ì°¨ ë¹„ìœ¨(MAPE)": np.mean(np.abs((y - y_pred) / y) * 100),
        "í‰ê·  ë¹„ìœ¨ ì˜¤ì°¨(MPE)": np.mean((y - y_pred) / y * 100)
    }

    print("[íšŒê·€ë¶„ì„ ì„±ëŠ¥í‰ê°€]")
    result_df = DataFrame([result], index=["ë°ì´í„°"])
    my_pretty_table(result_df)
    
    if report:
        print("")
        my_linear_regrassion_report(fit, x, y, order)
        
    # ì‹œê°í™”
    if use_plot:
        for i, v in enumerate(xnames):
            plt.figure(figsize=figsize, dpi=dpi)


            if degree -1 :
                sb.scatterplot(x=x[v], y=y, label='ê´€ì¸¡ì¹˜')
                sb.scatterplot(x=x[v], y=y_pred, label='ì¶”ì •ì¹˜')
                
                t1 = my_trend(x[v], y, degree = degree)
                sb.lineplot(x=t1[0], y=t1[1], color='blue', linestyle='--', alpha=0.5)
                
                t2 = my_trend(x[v], y_pred, degree = degree)
                sb.lineplot(x=t2[0], y=t2[1], color='red', linestyle='--', alpha=0.7)
            else:
                sb.regplot(x=x[v], y=y, ci=95, label='ê´€ì¸¡ì¹˜')
                sb.regplot(x=x[v], y=y_pred, ci=0, label='ì¶”ì •ì¹˜')
            

            plt.title(f"{yname} vs {v}")
            plt.legend()
            plt.grid()

            plt.show()
            plt.close()
    
    # ì”ì°¨ ê°€ì • í™•ì¸  
    if resid_test:
        print("\n\n[ì”ì°¨ì˜ ê°€ì • í™•ì¸] ==============================")
        my_resid_test(x, y, y_pred, figsize=figsize, dpi=dpi)

    # ë„ì¶œëœ ê²°ê³¼ë¥¼ íšŒê·€ëª¨ë¸ ê°ì²´ì— í¬í•¨ì‹œí‚´
    fit.x = x
    fit.y = y
    fit.y_pred = y_pred
    fit.resid = y - y_pred

def my_linear_regrassion_report(fit: LinearRegression, x: DataFrame = None, y: Series = None, order : str = None) -> None:
    """ì„ í˜•íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ í•œë‹¤.

    Args:
        fit (LinearRegression): ì„ í˜•íšŒê·€ ê°ì²´
        x (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
    """
    print("[ì„ í˜•íšŒê·€ë¶„ì„ ê²°ê³¼ë³´ê³ ]")
    if x is None and y is None:
        x = fit.x
        y = fit.y
    
    y_pred = fit.predict(x)
    xnames = x.columns
    yname = y.name

    # ì”ì°¨
    resid = y - y_pred

    # ì ˆí¸ê³¼ ê³„ìˆ˜ë¥¼ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•©
    params = np.append(fit.intercept_, fit.coef_)

    # ê²€ì¦ìš© ë…ë¦½ë³€ìˆ˜ì— ìƒìˆ˜í•­ ì¶”ê°€
    design_x = x.copy()
    design_x.insert(0, 'ìƒìˆ˜', 1)

    dot = np.dot(design_x.T,design_x)   # í–‰ë ¬ê³±
    inv = np.linalg.inv(dot)            # ì—­í–‰ë ¬
    dia = inv.diagonal()                # ëŒ€ê°ì›ì†Œ

    # ì œê³±ì˜¤ì°¨
    MSE = (sum((y-y_pred)**2)) / (len(design_x)-len(design_x.iloc[0]))

    se_b = np.sqrt(MSE * dia)           # í‘œì¤€ì˜¤ì°¨
    ts_b = params / se_b                # tê°’

    # ê° ë…ë¦½ìˆ˜ì— ëŒ€í•œ pvalue
    p_values = [2*(1-t.cdf(np.abs(i),(len(design_x)-len(design_x.iloc[0])))) for i in ts_b]

    # VIF
    vif = [variance_inflation_factor(x, list(x.columns).index(v)) for i, v in enumerate(x.columns)]

    # í‘œì¤€í™” ê³„ìˆ˜
    train_df = x.copy()
    train_df[y.name] = y
    scaler = StandardScaler()
    std = scaler.fit_transform(train_df)
    std_df = DataFrame(std, columns=train_df.columns)
    std_x = std_df[xnames]
    std_y = std_df[yname]
    std_model = LinearRegression()
    std_fit = std_model.fit(std_x, std_y)
    beta = std_fit.coef_

    # ê²°ê³¼í‘œ êµ¬ì„±í•˜ê¸°
    result_df = DataFrame({
        "ì¢…ì†ë³€ìˆ˜": [yname] * len(xnames),
        "ë…ë¦½ë³€ìˆ˜": xnames,
        "B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)": np.round(params[1:], 4),
        "í‘œì¤€ì˜¤ì°¨": np.round(se_b[1:], 3),
        "Î²(í‘œì¤€í™” ê³„ìˆ˜)": np.round(beta, 3),
        "t": np.round(ts_b[1:], 3),
        "ìœ ì˜í™•ë¥ ": np.round(p_values[1:], 3),
        "VIF": vif,
    })
    if order:
        order = order.upper()
        if order == 'V':
            result_df.sort_values('VIF',inplace=True)
        elif  order == 'P':
            result_df.sort_values('ìœ ì˜í™•ë¥ ',inplace=True)
        #result_df
    my_pretty_table(result_df)
        
    resid = y - y_pred        # ì”ì°¨
    dw = durbin_watson(resid)               # ë”ë¹ˆ ì™“ìŠ¨ í†µê³„ëŸ‰
    r2 = r2_score(y, y_pred)  # ê²°ì •ê³„ìˆ˜(ì„¤ëª…ë ¥)
    rowcount = len(x)                # í‘œë³¸ìˆ˜
    featurecount = len(x.columns)    # ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜

    # ë³´ì •ëœ ê²°ì •ê³„ìˆ˜
    adj_r2 = 1 - (1 - r2) * (rowcount-1) / (rowcount-featurecount-1)

    # fê°’
    f_statistic = (r2 / featurecount) / ((1 - r2) / (rowcount - featurecount - 1))

    # Prob (F-statistic)
    p = 1 - f.cdf(f_statistic, featurecount, rowcount - featurecount - 1)

    tpl = f"ğ‘…^2({r2:.3f}), Adj.ğ‘…^2({adj_r2:.3f}), F({f_statistic:.3f}), P-value({p:.4g}), Durbin-Watson({dw:.3f})"
    print(tpl, end="\n\n")

    # ê²°ê³¼ë³´ê³ 
    tpl = f"{yname}ì— ëŒ€í•˜ì—¬ {','.join(xnames)}ë¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼, ì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜{'í•˜ë‹¤' if p <= 0.05 else 'í•˜ì§€ ì•Šë‹¤'}(F({len(x.columns)},{len(x.index)-len(x.columns)-1}) = {f_statistic:0.3f}, p {'<=' if p <= 0.05 else '>'} 0.05)."

    print(tpl, end = '\n\n')

    # ë…ë¦½ë³€ìˆ˜ ë³´ê³ 
    for n in xnames:
        item = result_df[result_df['ë…ë¦½ë³€ìˆ˜'] == n]
        coef = item['B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)'].values[0]
        pvalue = item['ìœ ì˜í™•ë¥ '].values[0]

        s = f"{n}ì˜ íšŒê·€ê³„ìˆ˜ëŠ” {coef:0.3f}(p {'<=' if pvalue <= 0.05 else '>'} 0.05)ë¡œ, {yname}ì— ëŒ€í•˜ì—¬ {'ìœ ì˜ë¯¸í•œ' if pvalue <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•Šì€'} ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤."

        print(s)
        
    print("")

    # ë„ì¶œëœ ê²°ê³¼ë¥¼ íšŒê·€ëª¨ë¸ ê°ì²´ì— í¬í•¨ì‹œí‚´ --> ê°ì²´ íƒ€ì…ì˜ íŒŒë¼ë¯¸í„°ëŠ” ì°¸ì¡°ë³€ìˆ˜ë¡œ ì „ë‹¬ë˜ë¯€ë¡œ fit ê°ì²´ì— í¬í•¨ëœ ê²°ê³¼ê°’ë“¤ì€ ì´ í•¨ìˆ˜ ì™¸ë¶€ì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤.
    fit.r2 = r2
    fit.adj_r2 = adj_r2
    fit.f_statistic = f_statistic
    fit.p = p
    fit.dw = dw
        
def my_resid_normality(y: Series, y_pred: Series) -> None:
    """MSEê°’ì„ ì´ìš©í•˜ì—¬ ì”ì°¨ì˜ ì •ê·œì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
    """
    mse = mean_squared_error(y, y_pred)
    resid = y - y_pred
    mse_sq = np.sqrt(mse)

    r1 = resid[ (resid > -mse_sq) & (resid < mse_sq)].count() / resid.count() * 100
    r2 = resid[ (resid > -2*mse_sq) & (resid < 2*mse_sq)].count() / resid.count() * 100
    r3 = resid[ (resid > -3*mse_sq) & (resid < 3*mse_sq)].count() / resid.count() * 100

    mse_r = [r1, r2, r3]
    
    print(f"ë£¨íŠ¸ 1MSE êµ¬ê°„ì— í¬í•¨ëœ ì”ì°¨ ë¹„ìœ¨: {r1:1.2f}% ({r1-68})")
    print(f"ë£¨íŠ¸ 2MSE êµ¬ê°„ì— í¬í•¨ëœ ì”ì°¨ ë¹„ìœ¨: {r2:1.2f}% ({r2-95})")
    print(f"ë£¨íŠ¸ 3MSE êµ¬ê°„ì— í¬í•¨ëœ ì”ì°¨ ë¹„ìœ¨: {r3:1.2f}% ({r3-99})")
    
    normality = r1 >= 68 and r2 >= 95 and r3 >= 99
    print(f"ì”ì°¨ì˜ ì •ê·œì„± ê°€ì • ì¶©ì¡± ì—¬ë¶€: {normality}")

def my_resid_equal_var(x: DataFrame, y: Series, y_pred: Series) -> None:
    """ì”ì°¨ì˜ ë“±ë¶„ì‚°ì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        x (DataFrame): ë…ë¦½ë³€ìˆ˜
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
    """
    # ë…ë¦½ë³€ìˆ˜ ë°ì´í„° í”„ë ˆì„ ë³µì‚¬
    x_copy = x.copy()
    
    # ìƒìˆ˜í•­ ì¶”ê°€
    x_copy.insert(0, "const", 1)
    
    # ì”ì°¨ êµ¬í•˜ê¸°
    resid = y - y_pred
    
    # ë“±ë¶„ì‚°ì„± ê²€ì •
    bs_result = het_breuschpagan(resid, x_copy)
    bs_result_df = DataFrame(bs_result, columns=['values'], index=['statistic', 'p-value', 'f-value', 'f p-value'])

    print(f"ì”ì°¨ì˜ ë“±ë¶„ì‚°ì„± ê°€ì • ì¶©ì¡± ì—¬ë¶€: {bs_result[1] > 0.05}")
    my_pretty_table(bs_result_df)

def my_resid_independence(y: Series, y_pred: Series) -> None:
    """ì”ì°¨ì˜ ë…ë¦½ì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
    """
    dw = durbin_watson(y - y_pred)
    print(f"Durbin-Watson: {dw}, ì”ì°¨ì˜ ë…ë¦½ì„± ê°€ì • ë§Œì¡± ì—¬ë¶€: {1.5 < dw < 2.5}")
    
def my_resid_test(x: DataFrame, y: Series, y_pred: Series, figsize: tuple=(10, 4), dpi: int=150) -> None:
    """ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        x (Series): ë…ë¦½ë³€ìˆ˜
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
    """

    # ì”ì°¨ ìƒì„±
    resid = y - y_pred
    
    print("[ì”ì°¨ì˜ ì„ í˜•ì„± ê°€ì •]")
    my_residplot(y, y_pred, lowess=True, figsize=figsize, dpi=dpi)
    
    print("\n[ì”ì°¨ì˜ ì •ê·œì„± ê°€ì •]")
    my_qqplot(y, figsize=figsize, dpi=dpi)
    my_residplot(y, y_pred, mse=True, figsize=figsize, dpi=dpi)
    my_resid_normality(y, y_pred)
    
    print("\n[ì”ì°¨ì˜ ë“±ë¶„ì‚°ì„± ê°€ì •]")
    my_resid_equal_var(x, y, y_pred)
    
    print("\n[ì”ì°¨ì˜ ë…ë¦½ì„± ê°€ì •]")
    my_resid_independence(y, y_pred)