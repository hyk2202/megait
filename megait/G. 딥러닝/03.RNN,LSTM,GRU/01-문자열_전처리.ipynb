{"cells":[{"cell_type":"markdown","metadata":{"id":"K4E14CMWEMVM"},"source":["# 문자열 데이터 전처리\n","\n","## 워드 임베딩(Word Embedding)\n","\n","컴퓨터가 자연어를 이해하고, 효율적으로 처리하게 하기 위해서는 컴퓨터가 이해할 수 있도록 자연어를 적절히 변환할 필요가 있다.\n","\n","워드 임베딩(Word Embedding)은 단어를 벡터로 표현하는 방법으로, 단어를 희소 표현이 개선된 형태인 밀집 표현으로 변환한다.\n","\n","### 1. 희소 표현 (원-핫 백터)\n","\n","원-핫 인코딩과 비슷한 방법\n","\n","표현하고자 하는 단어의 인덱스 값만 1이고 나머지 인덱스에는 전부 0으로 표현되는 백터 방법으로 원-핫 백터라고도 한다.\n","\n","> `G-01-(2)-09-비정형데이터-분류` 예제에서 소개한 **문서단위행렬**\n","\n","#### 예시문장 두 개\n","\n","```\n","머신러닝 공부는 재미있다.\n","머신러닝은 유용하다.\n","```\n","\n","#### 형태소 분석\n","\n","위 의 두 예문에서 명사인 단어만 추출한다면 다음과 같다.\n","\n","```\n","머신러닝, 공부, 재미\n","머신러닝, 유용\n","```\n","\n","#### 토큰화\n","\n","각 단어에 인덱스 번호를 적용한다면 아래와 같이 표현할 수 있을 것이다.\n","\n","```\n","0, 1, 2\n","0, 3\n","```\n","\n","#### 희소 표현\n","\n","전체 단어의 수가 `4`개 이므로 각각의 단어를 4개의 원소를 갖는 리스트 안에서 one-hot 인코딩으로 표현한다.\n","\n","```\n","[ [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0] ]\n","[ [1, 0, 0, 0], [0, 0, 0, 1] ]\n","```\n","\n","#### 희소 표현의 한계\n","\n","단어의 개수가 늘어나면 벡터의 차원이 한없이 커진다.\n","\n","여러 문장을 통해 얻어진 단어가 10,000개이고 그 중에서 5개의 단어로 구성된 문장이라면 벡터의 총 길이는 50,000이 되게 된다.\n","\n","그러므로 공간적인 낭비를 가져온다.\n","\n","\n","### 2. 밀집표현\n","\n","희소 표현의 반대\n","\n","벡터의 차원을 단어 집합의 크기로 결정하지 않고 분석가가 설정한 임의의 값으로 모든 단어 벡터의 차원을 맞춘다.\n","\n","`0`과 `1`이 아닌 실수를 원소로 갖는다.\n","\n","### 앞에서 제시한 예시 문장에서 `머신러닝`이라는 단어의 예시\n","\n","희소표현 → `[1, 0, 0, 0]`\n","\n","벡터의 차원을 2로 설정한 밀집표현 → `[1.8, -0.4]`\n","\n","> 수치 값은 임의의 값이다. 이와 같은 식으로 실수 형태로 표현된다는 것을 표현한 것일 뿐 실제 정확한 값은 아님을 이해하자.\n","\n","결과적으로 벡터의 차원이 조밀해졌다고 하여 밀집 벡터라고 부른다.\n","\n","### 3. 워드 임베딩\n","\n","단어를 밀집 벡터의 형태로 표현한 방법.\n","\n","케라스에서는 **토큰화** 한 단어 벡터를 `Embedding()` 함수에 전달하여 학습층을 쌓음으로서 적용할 수 있다.\n","\n","단어를 랜덤한 값을 갖는 밀집 벡터로 변환한 뒤에, 인공 신경망의 가중치를 학습하는 방식으로 단어 벡터를 학습한다.\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"1o9P-nnQFnUG"},"source":["## #01. 준비작업\n","\n","### [1] 패키지 가져오기"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3852,"status":"ok","timestamp":1694668432469,"user":{"displayName":"이광호","userId":"07837067054228516785"},"user_tz":-540},"id":"1tkd__BbEMVO"},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]},{"name":"stderr","output_type":"stream","text":["[scatterd] >WARNING> From c:\\Users\\leekh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["# 연결된 모듈이 업데이트 되면 즉시 자동 로드함\n","%load_ext autoreload\n","%autoreload 2\n","\n","import warnings\n","warnings.filterwarnings(action=\"ignore\")\n","\n","from helper.util import *\n","from helper.plot import *\n","from helper.tensor import *\n","\n","# 형태소 분석 엔진 -> Okt\n","# from konlpy.tag import Okt\n","\n","# 형태소 분석 엔진 -> Mecab\n","from konlpy.tag import Mecab\n","\n","# 문자열 토큰화 처리\n","from tensorflow.keras.preprocessing.text import Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"RKGvO-A3Eh0Z"},"source":["## #02. 문자열 전처리\n","\n","### [1] 문자열 토큰화\n","\n","학습 데이터에서 단어단위로 일련번호로 변환하는 처리\n","\n","#### (1) 토큰화 학습 데이터"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":418,"status":"ok","timestamp":1694668579082,"user":{"displayName":"이광호","userId":"07837067054228516785"},"user_tz":-540},"id":"rbraSH2kEMVP"},"outputs":[],"source":["train_text = [\"You are the Best\", \"You are the Nice\"]"]},{"cell_type":"markdown","metadata":{"id":"NNxEDqh0EMVP"},"source":["#### (2) 토큰화 객체 생성\n","\n","- `num_words` : 밀집 표현의 최대 벡터 길이를 지정 (= 최대 단어 수)\n","- `oov_token` : 학습 결과를 적용할 때 학습 데이터에 포함되지 않은 단어가 존재할 경우 대체할 단어."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1694668581321,"user":{"displayName":"이광호","userId":"07837067054228516785"},"user_tz":-540},"id":"VMZIcqnvEMVP"},"outputs":[{"data":{"text/plain":["<keras.src.preprocessing.text.Tokenizer at 0x22cc910bd10>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer = Tokenizer(num_words=10, oov_token=\"<OOV>\")\n","tokenizer"]},{"cell_type":"markdown","metadata":{"id":"JKMpJIohEMVP"},"source":["#### (3) 토큰화 학습하기"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":403,"status":"ok","timestamp":1694668602118,"user":{"displayName":"이광호","userId":"07837067054228516785"},"user_tz":-540},"id":"UUvXN9yZEMVQ","outputId":"a3b43dc2-a75a-429a-db6e-cf63763b40b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'<OOV>': 1, 'you': 2, 'are': 3, 'the': 4, 'best': 5, 'nice': 6}\n"]}],"source":["# 토큰화 학습\n","tokenizer.fit_on_texts(train_text)\n","\n","# 각 단어에 부여된 인덱스 번호 확인\n","print(tokenizer.word_index)"]},{"cell_type":"markdown","metadata":{"id":"MlWZ5rmCEMVQ"},"source":["#### 학습 결과 적용하기\n","\n","토큰화를 학습한 모델에 새로운 단어를 적용하여 밀집 벡터로 변환한다."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1694668663216,"user":{"displayName":"이광호","userId":"07837067054228516785"},"user_tz":-540},"id":"3OpqJMGZEMVQ","outputId":"6c65dc7f-709b-4163-d745-1d2b6f12f9c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1, 3, 4, 5], [1, 3, 4, 6]]\n"]}],"source":["train_text = [\"We are the Best\", \"We are the Nice\"]\n","sequences = tokenizer.texts_to_sequences(train_text)\n","print(sequences)"]},{"cell_type":"markdown","metadata":{"id":"AsrEDQCZEMVR"},"source":["### [2] 한글 문장 토큰화\n","\n","#### (1) 토큰화 학습 데이터"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":524,"status":"ok","timestamp":1694668840804,"user":{"displayName":"이광호","userId":"07837067054228516785"},"user_tz":-540},"id":"VI0c1g71EMVR"},"outputs":[],"source":["poem = \"\"\"\n","흘러내린 머리카락이 흐린 호박빛 아래 빛난다.\n","난 유영한다. 차분하게 과거에 살면서 현재의 공기를 마신다.\n","가로등이 깜빡인다.\n","나도 깜빡여준다.\n","머리카락이 흩날린다.\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"maOuQ7VLEMVR"},"source":["#### (2) 불용어(stopwords) 목록\n","\n","분석에서 제외할 불필요한 단어에 대한 목록.\n","\n","개발자가 임의로 정하거나 불용어 사전을 웹에서 내려받아 사용할 수 있다."]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":425,"status":"ok","timestamp":1694668843076,"user":{"displayName":"이광호","userId":"07837067054228516785"},"user_tz":-540},"id":"SwVy0qJ7EMVR"},"outputs":[],"source":["stopwords = [\"난\", \"나\"]"]},{"cell_type":"markdown","metadata":{"id":"wBiRj6n0EMVS"},"source":["#### (3) Token (=형태소) 분리\n","\n","> `D-05-02-워드클라우드(한글)` 예제에서 형태소 분석에 대해 자세히 소개하고 있습니다.\n","\n","- 형태소란 문법적으로 더 이상 나눌 수 없는 언어 요소.\n","- 형태소 분리는 단어를 품사별로 구별하는 것을 말한다.\n","- 영어의 경우 각 단어로 나누면 되지만 한글의 경우 복잡한 처리 과정을 거쳐야 하기 때문에 별도의 라이브러리를 적용해야 한다. (konlpy, mecab 등)\n","- 머신러닝에서는 문장에서 명사만을 추출하는 것을 목표로 한다."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":414,"status":"ok","timestamp":1694668846249,"user":{"displayName":"이광호","userId":"07837067054228516785"},"user_tz":-540},"id":"SUxQVMtGEMVS","outputId":"b45769d0-a90c-43d0-eac8-2ba4f9c48df3"},"outputs":[{"name":"stdout","output_type":"stream","text":["['머리카락', '호박', '빛', '아래', '난', '유영', '과거', '현재', '공기', '가로등', '나', '머리카락']\n"]}],"source":["if sys.platform == \"win32\":\n","    mecab = Mecab(dicpath=\"C:\\\\mecab\\\\mecab-ko-dic\")\n","else:\n","    mecab = Mecab()\n","    \n","nouns = mecab.nouns(poem)\n","print(nouns)"]},{"cell_type":"markdown","metadata":{"id":"IGTv99NjEMVS"},"source":["#### (4) 추출된 명사에서 불용어를 제외한 새로운 리스트 만들기"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1694668922604,"user":{"displayName":"이광호","userId":"07837067054228516785"},"user_tz":-540},"id":"tEfzq0NlEMVS","outputId":"d9487ef0-105e-43db-f475-cebecc7d320f"},"outputs":[{"data":{"text/plain":["['머리카락', '호박', '빛', '아래', '유영', '과거', '현재', '공기', '가로등', '머리카락']"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["train_text = [x for x in nouns if x not in stopwords]\n","train_text"]},{"cell_type":"markdown","metadata":{"id":"Wt64ZrdqEMVS"},"source":["#### (5) 토큰화 수행"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":419,"status":"ok","timestamp":1694668925079,"user":{"displayName":"이광호","userId":"07837067054228516785"},"user_tz":-540},"id":"-NfQJN7mEMVS","outputId":"db1b022b-20ad-4133-87a0-33f12ecd2c22"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'<OOV>': 1, '머리카락': 2, '호박': 3, '빛': 4, '아래': 5, '유영': 6, '과거': 7, '현재': 8, '공기': 9, '가로등': 10}\n"]}],"source":["tokenizer = Tokenizer(num_words=len(nouns), oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(train_text)\n","print(tokenizer.word_index)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
